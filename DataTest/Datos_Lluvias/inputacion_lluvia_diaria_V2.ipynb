{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e7b73e-386f-4b15-b1bb-5cff59d6743b",
   "metadata": {},
   "source": [
    "# Librerías\n",
    "Nota: Si faltan paquetes, instala con !pip install <paquete> en otra celda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c7cb2e-f53d-4009-8997-f5a2f044bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from math import radians, sin, cos, sqrt, atan2\n",
    "from scipy.spatial.distance import cdist\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "\n",
    "from geopy.distance import great_circle  # O geodesic para mayor precisión\n",
    "import folium  # Asumiendo instalado\n",
    "\n",
    "import os\n",
    "import pprint\n",
    "\n",
    "import pmdarima as pm\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1926de4",
   "metadata": {},
   "source": [
    "# ¡¡¡ADVERTENCIA!!!\n",
    "\n",
    "## Este codigo solo se corre la primera vez, posterior a eso el resultado se guardara en un txt. para recuperar el dataframe generado en caso de que se corra esta linea por segunda vez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64935747",
   "metadata": {},
   "source": [
    "### Configuración manual de las estaciones (agrega solo las que tengas)\n",
    "station_files = {'05001': {'file': '05001.txt', 'lat': 27.92916667, 'lon': -100.575}, '05005': {'file': '05005.txt', 'lat': 26.84138889, 'lon': -100.7602778}, '05030': {'file': '05030.txt', 'lat': 27.51888889, 'lon': -100.6197222}, '05045': {'file': '05045.txt', 'lat': 27.61388889, 'lon': -100.725}, '05189': {'file': '05189.txt', 'lat': 27.59444444, 'lon': -100.7744444}, '05210': {'file': '05210.txt', 'lat': 26.81666667, 'lon': -100.7333333}, '19021': {'file': '19021.txt', 'lat': 26.49138889, 'lon': -100.0583333}, '19024': {'file': '19024.txt', 'lat': 27.23833333, 'lon': -100.1313889}, '19028': {'file': '19028.txt', 'lat': 27.07805556, 'lon': -100.4908333}, '19055': {'file': '19055.txt', 'lat': 27.42916667, 'lon': -100.3738889}, '19070': {'file': '19070.txt', 'lat': 27.03333333, 'lon': -100.5166667}, '19071': {'file': '19071.txt', 'lat': 26.49333333, 'lon': -99.52416667}, '19072': {'file': '19072.txt', 'lat': 27.28333333, 'lon': -100.0833333}, '19077': {'file': '19077.txt', 'lat': 26.65972222, 'lon': -99.98694444}, '19084': {'file': '19084.txt', 'lat': 26.5, 'lon': -100.0833333}, '19097': {'file': '19097.txt', 'lat': 27.70222222, 'lon': -99.75944444}, '19107': {'file': '19107.txt', 'lat': 27.53388889, 'lon': -99.97583333}, '19125': {'file': '19125.txt', 'lat': 27.43222222, 'lon': -99.80166667}, '19127': {'file': '19127.txt', 'lat': 26.88277778, 'lon': -99.82694444}, '19133': {'file': '19133.txt', 'lat': 26.50055556, 'lon': -100.1538889}, '19152': {'file': '19152.txt', 'lat': 27.06388889, 'lon': -100.0133333}, '19166': {'file': '19166.txt', 'lat': 27.17194444, 'lon': -100.4105556}, '19199': {'file': '19199.txt', 'lat': 26.56972222, 'lon': -99.79638889}, '28065': {'file': '28065.txt', 'lat': 27.48638889, 'lon': -99.50805556}, '28067': {'file': '28067.txt', 'lat': 26.58333333, 'lon': -99.18333333}, 'EU_Benavides2': {'file': 'EU_Benavides2.txt', 'lat': 27.596882, 'lon': -98.416414}, 'EU_Encinal1': {'file': 'EU_Encinal1.txt', 'lat': 27.977397, 'lon': -99.384733}, 'EU_Freer1': {'file': 'EU_Freer1.txt', 'lat': 27.872239, 'lon': -98.617604}, 'EU_Laredo1': {'file': 'EU_Laredo1.txt', 'lat': 27.533356, 'lon': -99.46666}, 'EU_Laredo2': {'file': 'EU_Laredo2.txt', 'lat': 27.568326, 'lon': -99.432792}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15b83d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 1. Inicializamos el diccionario que contendrá todos los datos.\n",
    "station_files = {}\n",
    "\n",
    "# 2. Obtenemos la lista de todos los archivos que terminan en .txt en la carpeta actual.\n",
    "lista_archivos_txt = [archivo for archivo in os.listdir('.') if archivo.endswith('.txt')]\n",
    "\n",
    "# 3. Recorremos cada archivo de la lista para procesarlo.\n",
    "for nombre_archivo in lista_archivos_txt:\n",
    "    lat = None\n",
    "    lon = None\n",
    "\n",
    "    try:\n",
    "        # Usamos 'with open' para asegurar que el archivo se cierre automáticamente.\n",
    "        with open(nombre_archivo, 'r', encoding='utf-8') as f:\n",
    "            # Leemos cada línea del archivo.\n",
    "            for linea in f:\n",
    "                # 4. Buscamos la palabra 'LATITUD' en la línea.\n",
    "                if 'LATITUD' in linea:\n",
    "                    # Si la encuentra, extraemos el número.\n",
    "                    # Pasos:\n",
    "                    # 1. Divide la línea en dos por el ':' -> ' 27.48638889 ° '\n",
    "                    # 2. .strip() quita espacios al inicio y final -> '27.48638889 °'\n",
    "                    # 3. .split(' ')[0] divide por espacio y toma la primera parte -> '27.48638889'\n",
    "                    valor_lat = linea.split(':')[1].strip().split(' ')[0]\n",
    "                    lat = float(valor_lat)\n",
    "\n",
    "                # 5. Hacemos lo mismo para la 'LONGITUD'.\n",
    "                elif 'LONGITUD' in linea:\n",
    "                    valor_lon = linea.split(':')[1].strip().split(' ')[0]\n",
    "                    lon = float(valor_lon)\n",
    "\n",
    "                # Si ya encontramos ambos valores, dejamos de leer el archivo.\n",
    "                if lat is not None and lon is not None:\n",
    "                    break\n",
    "\n",
    "        # 6. Si se extrajeron ambos datos, los agregamos al diccionario.\n",
    "        if lat is not None and lon is not None:\n",
    "            # El 'id' de la estación será el nombre del archivo sin '.txt'.\n",
    "            station_id = nombre_archivo.split('.')[0]\n",
    "            station_files[station_id] = {\n",
    "                'file': nombre_archivo,\n",
    "                'lat': lat,\n",
    "                'lon': lon\n",
    "            }\n",
    "        else:\n",
    "            print(f\"ADVERTENCIA: No se encontraron lat/lon en el archivo '{nombre_archivo}'.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR al procesar el archivo '{nombre_archivo}': {e}\")\n",
    "\n",
    "# 7. Finalmente, imprimimos el diccionario resultante con un formato legible.\n",
    "print(\"\\n--- Diccionario Final Generado ---\")\n",
    "pprint.pprint(station_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82027996",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(station_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9735e6b-6a11-4b73-af9f-ec75429a9612",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "map_center = [25.682500, -100.266944]  #--->  colocar coordenadas del lugar donde se centrara el mapa\n",
    "m = folium.Map(location=map_center, zoom_start=10)\n",
    "for sid, info in station_files.items():\n",
    "    folium.Marker([info['lat'], info['lon']], popup=sid).add_to(m)\n",
    "\n",
    "m.save('stations_map1.html')  # Abre en browser para interactividad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83172f0",
   "metadata": {},
   "source": [
    "# ¡¡¡fin de la ADVERTENCIA!!!\n",
    "\n",
    "\n",
    "# posterior a ejecuar la seccion anterior ya se puede limpiar los TXT para ejecutarlos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625c2165-6812-41d1-b5cf-06a127ba0ebb",
   "metadata": {},
   "source": [
    "# Distancia geográfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941f6b5c-f7ed-4d20-87e7-532c2cb263d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta funcios emplea la libreria Geopy para obtener un calculo \n",
    "# mucho mas preciso de distancias, considerando el metodo Haversine\n",
    "\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    point1 = (lat1, lon1)\n",
    "    point2 = (lat2, lon2)\n",
    "    return great_circle(point1, point2).km  # Devuelve distancia en km usando Haversine internamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafd5830-8791-4fed-9baf-a3b800c83d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# esta funcion ya no se utiliza en el codigo,\n",
    "# se emplea la libreria geopy en su lugar, \n",
    "# por mayor presicion al usar el metodo Haversine\n",
    "def haversine_distance(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"\n",
    "    esta primera seccion implementa la fórmula de Haversine, un método geométrico preciso para calcular la distancia\n",
    "    más corta entre dos puntos sobre una esfera (en este caso, la Tierra aproximada como una esfera).\n",
    "    Toma como entrada las coordenadas de dos puntos: lat1, lon1 (latitud y longitud del primer punto)\n",
    "    y lat2, lon2 (latitud y longitud del segundo punto), todas en grados. Devuelve la distancia en\n",
    "    kilómetros, lo que permite cuantificar la relación espacial entre estaciones\n",
    "    \"\"\"\n",
    "    # Radio de la Tierra en km\n",
    "    R = 6371.0  \n",
    "    \n",
    "    # Convierte a radianes la latitud y longitud de las cordenadas esfericas\n",
    "    lat1_rad, lon1_rad = radians(lat1), radians(lon1)\n",
    "    lat2_rad, lon2_rad = radians(lat2), radians(lon2)\n",
    "    \n",
    "    # Calcula la diferencia entre los puntos de referencia\n",
    "    dlat = lat2_rad - lat1_rad\n",
    "    dlon = lon2_rad - lon1_rad\n",
    "    \n",
    "    # Esta expresión representa la \"haversine\" de la diferencia angular entre\n",
    "    # los puntos. Utiliza funciones trigonométricas (seno y coseno) para modelar\n",
    "    # la geometría esférica. El término sin(dlat / 2)**2 y sin(dlon / 2)**2 \n",
    "    # simplifica el cálculo evitando problemas de precisión cerca de los polos,\n",
    "    #mientras que cos(lat1_rad) * cos(lat2_rad) ajusta por la curvatura latitudinal.\n",
    "    a = sin(dlat / 2)**2 + cos(lat1_rad) * cos(lat2_rad) * sin(dlon / 2)**2\n",
    "    \n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cdba83-0b16-4665-8e64-84b3db6da63e",
   "metadata": {},
   "source": [
    "# Parsin, limpieza y ordenado de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff9f877-05a4-417d-b890-55817afd2063",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Esta función actúa como el primer paso para procesar los datos crudos\n",
    "# de las estaciones meteorológicas almacenados en archivos de texto (.txt).\n",
    "# Su propósito principal es leer y transformar estos archivos en un formato\n",
    "# estructurado (DataFrame de pandas) que pueda ser utilizado por las funciones\n",
    "# posteriores de imputación, como idw_impute, mlr_impute y arima_impute\n",
    "\n",
    "def parse_station_data(file_path, station_id, lat, lon):\n",
    "    \"\"\"\n",
    "    Parsea un archivo .txt de estación meteorológica de CONAGUA, extrayendo FECHA y PRECIP.\n",
    "    Maneja valores 'NULO' como NaN.\n",
    "    \"\"\"\n",
    "    # Ubicacion donde guarda los datos generados\n",
    "    data = []\n",
    "    # revicion de los archivos y lectura de cada linea\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:  # Cambia a 'latin-1' si hay errores de encoding\n",
    "        lines = f.readlines()\n",
    "        parsing = False\n",
    "        # Identificacion de la FECHA y PRECIP dentro del contenido \n",
    "        for line in lines:\n",
    "            if 'FECHA' in line and 'PRECIP' in line:\n",
    "                parsing = True\n",
    "                continue\n",
    "            # Si la identifica procede a clasificar si hay espacio y verificar si existen los dos datos den cada linea, fecha y precipitacion\n",
    "            if parsing:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 2:\n",
    "                    # coloca a la fecha en la primera columna \n",
    "                    fecha = parts[0]\n",
    "                    # coloca a la precipiracion en la segunda columnan si\n",
    "                    # es nulo lo clasifica como Nan y lo integra como flotante, si da error lo pasa como Nan\n",
    "                    precip = parts[1] if parts[1] != 'NULO' else np.nan\n",
    "                    try:\n",
    "                        precip = float(precip)\n",
    "                    except ValueError:\n",
    "                        precip = np.nan\n",
    "                        #Agrega los elementos a la variable data, generada al principio \n",
    "                    data.append([fecha, precip, station_id, lat, lon])\n",
    "                    # Los transforma en un dataframe\n",
    "    df = pd.DataFrame(data, columns=['FECHA', 'PRECIP', 'station_id', 'lat', 'lon'])\n",
    "    # configura la fecha a datatime, si da error borra la fecha y suregistro de los datos\n",
    "    df['FECHA'] = pd.to_datetime(df['FECHA'], format='%Y-%m-%d', errors='coerce')  # Formato especificado para eliminar warnings\n",
    "    return df.dropna(subset=['FECHA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0ac4b7-d0a7-4fd9-ad17-6c6971dd4ce3",
   "metadata": {},
   "source": [
    "# Inputacion de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6f91ae-9b4a-466d-ba24-3c02f4ac015d",
   "metadata": {},
   "source": [
    "## Inverse Distance Weighting (IDW)\n",
    "\n",
    "Este método solo realiza la interpolación entre estaciones si existe valores dentro de la misma fecha, si este valor no existe no imputa y se pasa a analizar la siguiente fecha y datos existentes entre las estaciones para esa misma fecha, dentro de un rango límite de 50 km, lo que puede ser ajustado para considerar únicamente estaciones dentro de un rango aceptable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862656bd-d778-4647-b1b8-2b8cab3210e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idw_impute(target_df, other_dfs, power=2, max_dist=50):\n",
    "    \"\"\"\n",
    "    Imputa valores faltantes en PRECIP usando Inverse Distance Weighting (IDW).\n",
    "    El objetivo es imputar los valores faltantes en target_df['PRECIP'] utilizando\n",
    "    una media ponderada de los valores de precipitación de otras estaciones,\n",
    "    donde el peso de cada estación se calcula como el inverso de la distancia\n",
    "    elevada a power, restringido por max_dist. Esto refleja la idea de que las\n",
    "    estaciones más cercanas tienen mayor influencia en la estimación.\n",
    "    \"\"\"\n",
    "    # Se realiza una combinacion de los dataframe tanto del DF objetivo como el resto\n",
    "    # concatena uniendo todos y los acomoda por fechas\n",
    "    all_dfs = [target_df] + other_dfs\n",
    "    combined = pd.concat(all_dfs).sort_values('FECHA')\n",
    "\n",
    "    # Itera sobre las filas identificando el idx y los valores de cada fila (row)\n",
    "    for idx, row in target_df[target_df['PRECIP'].isna()].iterrows():\n",
    "        # Obtiene fecha y cordenadas de cada uno de los elemntos de la fila \n",
    "        fecha = row['FECHA']\n",
    "        lat_t, lon_t = row['lat'], row['lon']\n",
    "\n",
    "        # Realiza una busqueda de caondidatos que presenten nan en precipitacion y que tenga datos valodos en el resto de estaciones\n",
    "        # si no existe algun valoe valodo en ninguna otra estacion el codigo pasa a analizar el siguiente renglon\n",
    "        candidates = combined[(combined['FECHA'] == fecha) & (combined['station_id'] != row['station_id']) & combined['PRECIP'].notna()]\n",
    "        if candidates.empty:\n",
    "            continue\n",
    "        # Se realiza el calculo de las sitancias desde la estacion a la estacion vecina con datos empleando la funcion creaqda al principio\n",
    "        # en el apartado Distancia Geografica, si no hay candidatos en esa fecha salta al sigueinte valor para inputar\n",
    "        dists = candidates.apply(lambda r: calculate_distance(lat_t, lon_t, r['lat'], r['lon']), axis=1)\n",
    "        candidates = candidates[dists <= max_dist]\n",
    "        if candidates.empty:\n",
    "            continue\n",
    "        # asignacion de posos para cada estacion en relacion a el inverso de la distancia elevada al factor power=2\n",
    "        weights = 1 / (dists ** power)\n",
    "        # Calcula la media ponderada de los valores de precipitación de las candidatas, normalizando por la suma de los pesos.\n",
    "        imputed = np.sum(weights * candidates['PRECIP']) / np.sum(weights)\n",
    "        #Asigna el valor inputado\n",
    "        target_df.at[idx, 'PRECIP'] = max(0, imputed)\n",
    "\n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6791f4e-7d11-4aec-8196-a6a8169f2709",
   "metadata": {},
   "source": [
    "# Regresión Lineal Múltiple (MLR)\n",
    "\n",
    "Integración en main_imputation: Actúa como un paso secundario tras idw_impute. Si quedan NaN después de la imputación espacial (e.g., en 2025-05-13 de '19185'), mlr_impute intenta completarlos usando correlaciones con otras estaciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a811a5-31ed-4cca-8bc3-39307f27636a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlr_impute(target_df, other_dfs):\n",
    "    \"\"\"\n",
    "    Imputa usando Regresión Lineal Múltiple (MLR).\n",
    "    utilizando los valores de precipitación de otras\n",
    "    estaciones (other_dfs) como predictores. \n",
    "    Este método asume una relación lineal entre \n",
    "    las precipitaciones de la estación objetivo y\n",
    "    las vecinas, ofreciendo una imputación basada en \n",
    "    correlaciones estadísticas cuando los datos \n",
    "    espaciales de idw_impute no son suficientes.\n",
    "    \"\"\"\n",
    "    # Une los dataframe de otras estaciones al marge renombrando las columnas por id de sitio PRECIP_0, PRECIP_1, etc\n",
    "    # estas colubnas las emplea como variables independientes, precervando fechas y agregando Nan en donde no se cuenta con datos\n",
    "    merged = target_df.set_index('FECHA')\n",
    "    for i, df in enumerate(other_dfs):\n",
    "        merged = merged.join(df.set_index('FECHA')[['PRECIP']].rename(columns={'PRECIP': f'PRECIP_{i}'}), how='left')\n",
    "    # Selecciona solo las filas donde no hay valores faltantes en ninguna columna, para entrenar el modelo.\n",
    "    complete = merged.dropna()\n",
    "    if len(complete) < 10:\n",
    "        merged['PRECIP'] = merged['PRECIP'].fillna(merged['PRECIP'].mean())\n",
    "        return merged.reset_index()\n",
    "    # Define las variables independientes como todas las columnas excepto PRECIP\n",
    "    X = complete.drop('PRECIP', axis=1)\n",
    "    # Define la variable dependiente como PRECIP\n",
    "    y = complete['PRECIP']\n",
    "    # se realiza el entrenamiento del modelo empleando la libreria scikit-learn\n",
    "    model = LinearRegression().fit(X, y)\n",
    "\n",
    "    # Identifica las filas faltantes\n",
    "    na_rows = merged[merged['PRECIP'].isna()]\n",
    "    # Inputa el valor faltante identificado previamente\n",
    "    if not na_rows.empty:\n",
    "        X_na = na_rows.drop('PRECIP', axis=1).fillna(0)\n",
    "        #Predice los valores de PRECIP usando el modelo entrenado.\n",
    "        preds = model.predict(X_na)\n",
    "        # asigna la prediccion a las posiciones faltantes \n",
    "        merged.loc[merged['PRECIP'].isna(), 'PRECIP'] = np.maximum(0, preds)\n",
    "    \n",
    "    return merged.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e4559a-f67b-4955-a4a1-c9fe719aa3fb",
   "metadata": {},
   "source": [
    "# AutoRegressive Integrated Moving Average (ARIMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e7ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arima_impute_mejorado(df, gap_threshold=5):\n",
    "    \"\"\"\n",
    "    Imputa gaps temporales largos usando un modelo SARIMA óptimo encontrado por auto_arima.\n",
    "    \n",
    "    Este método es una versión mejorada que:\n",
    "    1. Utiliza pmdarima.auto_arima para encontrar el mejor orden (p,d,q).\n",
    "    2. Incorpora estacionalidad (P,D,Q,m=365) para capturar patrones anuales de lluvia.\n",
    "    3. Es más robusto al ajustarse dinámicamente a la estructura de cada serie temporal.\n",
    "    \"\"\"\n",
    "    df = df.sort_values('FECHA').reset_index(drop=True)\n",
    "    \n",
    "    # La detección de gaps se mantiene igual, es muy eficiente.\n",
    "    gaps = df['PRECIP'].isna().astype(int).groupby(df['PRECIP'].notna().cumsum()).sum()\n",
    "    long_gaps = gaps[gaps >= gap_threshold]\n",
    "\n",
    "    max_historico = df['PRECIP'].max() # Obtener el máximo conocido\n",
    "    \n",
    "    for gap_idx in long_gaps.index:\n",
    "        # La lógica para encontrar el inicio y fin del gap también se mantiene.\n",
    "        if gap_idx > 0:\n",
    "            start_idx = df[df['PRECIP'].notna().cumsum() == gap_idx - 1].index[-1] + 1\n",
    "        else:\n",
    "            start_idx = 0\n",
    "        \n",
    "        gap_size = long_gaps[gap_idx]\n",
    "        end_idx = start_idx + gap_size - 1\n",
    "\n",
    "        before = df.loc[:start_idx-1, 'PRECIP'].dropna()\n",
    "        after = df.loc[end_idx+1:, 'PRECIP'].dropna()\n",
    "\n",
    "        # --- INICIO DE LA MEJORA ---\n",
    "        \n",
    "        # Se necesita suficiente data para un modelo estacional (idealmente > 2*m)\n",
    "        # pero auto_arima puede manejar series más cortas. Aumentamos el umbral a 30.\n",
    "        if len(before) > 30:\n",
    "            try:\n",
    "                # Usar auto_arima para encontrar el mejor modelo SARIMA\n",
    "                sarima_model = pm.auto_arima(before,\n",
    "                                             start_p=1, start_q=1,\n",
    "                                             test='adf',       # test de estacionariedad\n",
    "                                             max_p=3, max_q=3,  # órdenes máximos a probar\n",
    "                                             m=365,            # ciclo estacional (diario)\n",
    "                                             d=None,           # dejar que encuentre d\n",
    "                                             seasonal=True,    # buscar modelo estacional\n",
    "                                             start_P=0,\n",
    "                                             D=1,              # D=1 suele funcionar para estacionalidad\n",
    "                                             trace=False,      # no imprimir cada paso\n",
    "                                             error_action='ignore',\n",
    "                                             suppress_warnings=True,\n",
    "                                             stepwise=True)    # búsqueda más rápida\n",
    "                \n",
    "                forecast = sarima_model.predict(n_periods=gap_size)\n",
    "                forecast_clipped = np.clip(forecast.values, 0, max_historico)#\n",
    "                df.loc[start_idx:end_idx, 'PRECIP'] = forecast_clipped #df.loc[start_idx:end_idx, 'PRECIP'] = np.maximum(0, forecast.values)\n",
    "\n",
    "            except Exception as e: # Fallback si auto_arima falla\n",
    "                print(f\"AutoARIMA falló para gap {gap_idx}, usando interpolación lineal. Error: {e}\")\n",
    "                df['PRECIP'].interpolate(method='linear', limit_direction='both', inplace=True)\n",
    "\n",
    "        # Pronóstico hacia atrás si no hay datos suficientes antes\n",
    "        elif len(after) > 30:\n",
    "            # La lógica es la misma, pero con la serie invertida (after[::-1])\n",
    "            sarima_model = pm.auto_arima(after[::-1], seasonal=True, m=365, suppress_warnings=True, error_action='ignore', stepwise=True)\n",
    "            forecast = sarima_model.predict(n_periods=gap_size)\n",
    "            forecast_clipped = np.clip(forecast.values[::-1], 0, max_historico)\n",
    "            df.loc[start_idx:end_idx, 'PRECIP'] = forecast_clipped #df.loc[start_idx:end_idx, 'PRECIP'] = np.maximum(0, forecast.values[::-1])\n",
    "            \n",
    "            \n",
    "        \n",
    "        # Fallback si no hay suficientes datos en ninguna dirección\n",
    "        else:\n",
    "            df.loc[start_idx:end_idx, 'PRECIP'] = df['PRECIP'].mean()\n",
    "\n",
    "    # Interpolación final para cualquier NaN restante\n",
    "    df['PRECIP'] = df['PRECIP'].interpolate(method='linear', limit_direction='both')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a2feda-fb74-4def-8dd9-3e8662673efb",
   "metadata": {},
   "source": [
    "def arima_impute(df, gap_threshold=5, order=(1,1,1)):\n",
    "    \"\"\"\n",
    "    Imputa gaps temporales largos usando ARIMA.\n",
    "    implementa el modelo ARIMA (AutoRegressive \n",
    "    Integrated Moving Average) para imputar gaps\n",
    "    largos de datos faltantes en df['PRECIP'], \n",
    "    enfocándose en patrones temporales. Es el \n",
    "    último recurso en el pipeline, utilizado \n",
    "    cuando los métodos espaciales (idw_impute)\n",
    "    y estadísticos (mlr_impute) no completan \n",
    "    todos los NaN.\n",
    "    \"\"\"\n",
    "    # se ordenan los datos por fechas \n",
    "    df = df.sort_values('FECHA')\n",
    "    # deteccion de gaps, identifica los Nan clasificandolos con valores de 1\n",
    "    # y valores validos en 0, los agrupa y calcula la longitud de cada GAP\n",
    "    gaps = df['PRECIP'].isna().astype(int).groupby(df['PRECIP'].notna().cumsum()).sum()\n",
    "    # Filtra gaps mayores o iguales a gap_threshold (default 5 días).\n",
    "    long_gaps = gaps[gaps >= gap_threshold]\n",
    "\n",
    "    # Procesamiento de Cada Gap Largo:\n",
    "    for gap_idx in long_gaps.index:\n",
    "        start = df[df['PRECIP'].notna().cumsum() == gap_idx - 1].index[-1] + 1 if gap_idx > 0 else 0\n",
    "        end = start + long_gaps[gap_idx] - 1\n",
    "        before = df.loc[:start-1, 'PRECIP'].dropna()\n",
    "        after = df.loc[end+1:, 'PRECIP'].dropna()\n",
    "\n",
    "        # imputa con ARIMA\n",
    "\n",
    "        # Si hay al menos 10 valores antes, entrena\n",
    "        # un modelo ARIMA con order=(1,1,1) (1 autorregresión,\n",
    "        # 1 diferenciación, 1 media móvil) y predice hacia adelante.\n",
    "        if len(before) > 10:\n",
    "            model = ARIMA(before, order=order).fit()\n",
    "            forecast = model.forecast(steps=long_gaps[gap_idx])\n",
    "            df.loc[start:end, 'PRECIP'] = np.maximum(0, forecast)\n",
    "        # Si no hay suficientes datos antes pero sí después,\n",
    "        # entrena un modelo con la serie invertida y predice hacia atrás.    \n",
    "        elif len(after) > 10:\n",
    "            model = ARIMA(after[::-1], order=order).fit()\n",
    "            forecast = model.forecast(steps=long_gaps[gap_idx])[::-1]\n",
    "            df.loc[start:end, 'PRECIP'] = np.maximum(0, forecast)\n",
    "        # Si no hay suficientes datos en ninguna dirección,\n",
    "        # usa la media de df['PRECIP'].mean() como fallback.\n",
    "        else:\n",
    "            df.loc[start:end, 'PRECIP'] = df['PRECIP'].mean()\n",
    "    #Asigna las predicciones, asegurando valores no negativos.\n",
    "    df['PRECIP'] = df['PRECIP'].interpolate(method='linear', limit_direction='both')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0e9d5b-23f7-47b4-aba9-616ad4fbfe01",
   "metadata": {},
   "source": [
    "# Iteración sobre las estaciones + orden jerárquico\n",
    "\n",
    "El objetivo es iterar sobre todas las estaciones, leer sus datos crudos, aplicar un enfoque jerárquico de imputación (IDW → MLR → ARIMA), y devolver un diccionario (imputed) con los DataFrames resultantes, donde los valores faltantes de precipitación (PRECIP) han sido estimados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af262c0c-d2b8-4a3d-9f72-dd41044818de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_imputation(station_files):\n",
    "    \"\"\"\n",
    "    Función principal: Procesa múltiples archivos de estaciones, aplica IDW/MLR y ARIMA sobre rango completo de fechas.\n",
    "    \"\"\"\n",
    "    dfs = {}\n",
    "    all_dates = set()  # Para recopilar todas las fechas únicas\n",
    "    \n",
    "    # Leer datos y recopilar fechas\n",
    "    for sid, info in station_files.items():\n",
    "        df = parse_station_data(info['file'], sid, info['lat'], info['lon'])\n",
    "        dfs[sid] = df\n",
    "        all_dates.update(df['FECHA'])\n",
    "    \n",
    "    # Rango global: min a max fecha, frecuencia diaria\n",
    "    if all_dates:\n",
    "        min_date = min(all_dates)\n",
    "        max_date = max(all_dates)\n",
    "        full_date_range = pd.date_range(start=min_date, end=max_date, freq='D')\n",
    "    else:\n",
    "        return {}  # No hay datos\n",
    "    \n",
    "    imputed = {}\n",
    "    for sid in dfs:\n",
    "        target = dfs[sid].set_index('FECHA').reindex(full_date_range).reset_index().rename(columns={'index': 'FECHA'})\n",
    "        target['station_id'] = sid  # Asignar station_id como constante para todas las filas\n",
    "        target['lat'] = station_files[sid]['lat']\n",
    "        target['lon'] = station_files[sid]['lon']\n",
    "        others = [dfs[s].set_index('FECHA').reindex(full_date_range).reset_index().rename(columns={'index': 'FECHA'}) for s in dfs if s != sid]\n",
    "        for i, o in enumerate(others):\n",
    "            o['station_id'] = dfs[list(dfs.keys())[i]]['station_id'].iloc[0]  # Asignar ID correcto con longitud adecuada\n",
    "        \n",
    "        # Primero IDW espacial\n",
    "        target = idw_impute(target, others)\n",
    "        \n",
    "        # Luego MLR si quedan NaNs\n",
    "        if target['PRECIP'].isna().any():\n",
    "            target = mlr_impute(target, others)\n",
    "        \n",
    "        # Finalmente ARIMA para gaps temporales\n",
    "        if target['PRECIP'].isna().any():\n",
    "            target = arima_impute_mejorado(target)\n",
    "        \n",
    "        imputed[sid] = target\n",
    "    \n",
    "    return imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a95188",
   "metadata": {},
   "source": [
    "Realizaremos la importacion y posterior limpieza de los archivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b200a0-985a-4f04-9f26-953089cf5f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la imputación\n",
    "results = main_imputation(station_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407e6150-4e7f-4c73-a73a-8b787753211f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo para visualizar resultados de una estación específica (ejemplo: 19185)\n",
    "#print(\"Resultados imputados para estación 19052:\")\n",
    "#print(results['19052'].head(10))  # Muestra las primeras 10 filas\n",
    "\n",
    "# Exportar a CSV para análisis posterior\n",
    "for sid, df in results.items():\n",
    "    df.to_csv(f'{sid}_imputado.csv', index=False)\n",
    "    print(f\"Archivo exportado: {sid}_imputado.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea98f7d0-afe8-4004-b9e7-2261043be533",
   "metadata": {},
   "source": [
    "# Explicación del Gráfico\n",
    "\n",
    "Líneas: La línea azul muestra los valores originales de PRECIP (con NaN donde había 'NULO'), mientras que la roja muestra los valores imputados.\n",
    "Puntos Verdes: Marcadores que indican dónde se imputaron valores, destacando visualmente los reemplazos de 'NULO'.\n",
    "Ejes: El eje X representa las fechas, y el Y la precipitación en mm. La rotación de etiquetas en X mejora la legibilidad.\n",
    "Personalización: Título, leyenda y rejilla facilitan la interpretación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d402add",
   "metadata": {},
   "source": [
    "# Atención  apartir de esta seccion es necesario identificar la estacion de trabajo siga las indicaciones marcadas en #  ---> prueba con estacion Monterrey 19049  25.682500, -100.266944"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8c13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_id = '19200' # ---> Coloque la estacion sobre la que se centraran los estadsiticos y analisis 19049\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1109bca-b1de-4ae9-888a-fa9f39a36259",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_center = [25.682500, -100.266944]  #--->  colocar coordenadas del lugar donde se centrara el mapa\n",
    "m = folium.Map(location=map_center, zoom_start=10)\n",
    "\n",
    "\n",
    "for sid, info in station_files.items():\n",
    "    folium.Marker([info['lat'], info['lon']], popup=sid).add_to(m)\n",
    "\n",
    "# ---> colocar clave de las estaciones point1 = mas alejada  point2 = la estacion de estudio \n",
    "point1 = (station_files['19169']['lat'], station_files['19169']['lon'])\n",
    "point2 = (station_files['19200']['lat'], station_files['19200']['lon'])\n",
    "dist = calculate_distance(*point1, *point2)  # O con Geopy\n",
    "folium.PolyLine([point1, point2], color=\"red\", weight=2.5, opacity=1, popup=f\"Dist: {dist:.2f} km\").add_to(m)\n",
    "\n",
    "m.save('stations_map.html')  # Abre en browser para interactividad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321bc7e3",
   "metadata": {},
   "source": [
    "# Atención"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03bf7cc",
   "metadata": {},
   "source": [
    "### Grafico con rango limitado a gusto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57461636-9dc4-4688-8e0e-ba2a117d1162",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = parse_station_data(station_files[station_id]['file'], station_id, station_files[station_id]['lat'], station_files[station_id]['lon'])\n",
    "df_imputed = results[station_id]\n",
    "\n",
    "\n",
    "df_original = df_original.set_index('FECHA')\n",
    "df_imputed = df_imputed.set_index('FECHA')\n",
    "combined_df = df_original.join(df_imputed['PRECIP'].rename('PRECIP_imputed'), how='outer')\n",
    "\n",
    "# Filtrar solo el año 2025\n",
    "combined_df_2025 = combined_df.loc['2009-01-01':'2011-12-31']  # ---> ajusta rango de fechas de observacion en el grafico\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(combined_df_2025.index, combined_df_2025['PRECIP'], 'b-', label='Original (NULO como NaN)', alpha=0.5, marker='o')\n",
    "plt.plot(combined_df_2025.index, combined_df_2025['PRECIP_imputed'], 'r-', label='Imputado', alpha=0.7, marker='o')\n",
    "\n",
    "plt.title(f'Comparación de Precipitación Original vs Imputada - Estación {station_id} (2025)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Precipitación (mm)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "nulos_idx = combined_df_2025['PRECIP'].isna()\n",
    "plt.scatter(combined_df_2025.index[nulos_idx], combined_df_2025['PRECIP_imputed'][nulos_idx], color='green', label='Nulos Imputados', zorder=5)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de04cb3",
   "metadata": {},
   "source": [
    "# Atención"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7be8c4f9",
   "metadata": {},
   "source": [
    "### grafico con rango completo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104390f8-1c68-490b-b5c8-bc586b02e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = parse_station_data(station_files[station_id]['file'], station_id, station_files[station_id]['lat'], station_files[station_id]['lon'])\n",
    "df_imputed = results[station_id]\n",
    "\n",
    "df_original = df_original.set_index('FECHA')\n",
    "df_imputed = df_imputed.set_index('FECHA')\n",
    "combined_df = df_original.join(df_imputed['PRECIP'].rename('PRECIP_imputed'), how='outer')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(combined_df.index, combined_df['PRECIP'], 'b-', label='Original (NULO como NaN)', alpha=0.5, marker='o')\n",
    "plt.plot(combined_df.index, combined_df['PRECIP_imputed'], 'r-', label='Imputado', alpha=0.7, marker='o')\n",
    "\n",
    "# Personalizar el gráfico\n",
    "plt.title(f'Comparación de Precipitación Original vs Imputada - Estación {station_id}')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Precipitación (mm)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "nulos_idx = combined_df['PRECIP'].isna()\n",
    "plt.scatter(combined_df.index[nulos_idx], combined_df['PRECIP_imputed'][nulos_idx], color='green', label='Nulos Imputados', zorder=5)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696f7879",
   "metadata": {},
   "source": [
    "### Grafico con Rango Completo de Datos Reales vs Nulos Actualizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f67a2-db87-44cf-91dd-7d9256720bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_original = parse_station_data(station_files[station_id]['file'], station_id, station_files[station_id]['lat'], station_files[station_id]['lon'])\n",
    "df_imputed = results[station_id]\n",
    "\n",
    "# Asegurar que las fechas estén alineadas\n",
    "df_original = df_original.set_index('FECHA')\n",
    "df_imputed = df_imputed.set_index('FECHA')\n",
    "combined_df = df_original.join(df_imputed['PRECIP'].rename('PRECIP_imputed'), how='outer')\n",
    "\n",
    "# Filtrar solo el año 2025\n",
    "combined_df_2025 = combined_df.loc['2020-01-01':'2021-12-31']\n",
    "\n",
    "# Separar datos reales (no nulos originales) y nulos actualizados (imputados donde había NaN)\n",
    "real_data = combined_df_2025['PRECIP'].dropna()  # Valores originales no nulos\n",
    "imputed_data = combined_df_2025[combined_df_2025['PRECIP'].isna()]['PRECIP_imputed']  # Valores imputados donde había nulos\n",
    "\n",
    "# Crear el gráfico\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Graficar datos reales\n",
    "plt.plot(real_data.index, real_data, 'bo-', label='Datos Reales', alpha=0.7, marker='o')\n",
    "\n",
    "# Graficar nulos actualizados\n",
    "plt.plot(imputed_data.index, imputed_data, 'ro-', label='Nulos Actualizados', alpha=0.7, marker='o')\n",
    "\n",
    "# Personalizar el gráfico\n",
    "plt.title(f'Datos Reales vs Nulos Actualizados - Estación {station_id} (2025)')\n",
    "plt.xlabel('Fecha')\n",
    "plt.ylabel('Precipitación (mm)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Añadir una línea de referencia para cero\n",
    "plt.axhline(y=0, color='k', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Opcional: Guardar el gráfico\n",
    "# plt.savefig(f'real_vs_imputed_{station_id}_2025.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce681f",
   "metadata": {},
   "source": [
    "## Evaluacion de distribuciones de ajuste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb675536-a3bd-415c-b39b-4f82e7475f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_best_distribution(data, distributions=None):\n",
    "    \"\"\"\n",
    "    Evalúa la mejor distribución que se ajusta a los datos no nulos.\n",
    "    Retorna el nombre de la mejor distribución y sus parámetros.\n",
    "    \"\"\"\n",
    "    if distributions is None:\n",
    "        distributions = [stats.gamma, stats.lognorm, stats.weibull_min, stats.expon]\n",
    "    \n",
    "    data = data.dropna().values  # Solo datos no nulos\n",
    "    data = data[data > 0]  # Precipitación >0 para distribuciones positivas\n",
    "    \n",
    "    if len(data) < 3:\n",
    "        return \"No suficiente datos para ajustar\", None\n",
    "    \n",
    "    results = {}\n",
    "    for dist in distributions:\n",
    "        params = dist.fit(data)\n",
    "        ks_stat, ks_pvalue = stats.kstest(data, dist.cdf, args=params)\n",
    "        results[dist.name] = (ks_stat, ks_pvalue, params)\n",
    "    \n",
    "    # Mejor distribución: Mayor p-value en KS-test (mejor ajuste)\n",
    "    best_dist_name = max(results, key=lambda k: results[k][1])\n",
    "    best_ks_stat, best_ks_pvalue, best_params = results[best_dist_name]\n",
    "    \n",
    "    return best_dist_name, best_params\n",
    "\n",
    "df_original = parse_station_data(station_files[station_id]['file'], station_id, station_files[station_id]['lat'], station_files[station_id]['lon'])\n",
    "best_dist_orig, params_orig = evaluate_best_distribution(df_original['PRECIP'])\n",
    "print(f\"Mejor distribución para originales de {station_id}: {best_dist_orig} con parámetros {params_orig}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e354df-5ecf-4f13-84a1-9dd77b3e33dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_imputed = results[station_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf0745",
   "metadata": {},
   "source": [
    "### Comparacion estadistica DR vs DR+Imputados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f202624-6cea-41b7-870c-53d27c686e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(data, label):\n",
    "    \"\"\"\n",
    "    Calcula estadísticos descriptivos.\n",
    "    \"\"\"\n",
    "    stats_dict = {\n",
    "        'Media': data.mean(),\n",
    "        'Mediana': data.median(),\n",
    "        'Desviación Estándar': data.std(),\n",
    "        'Skewness': data.skew(),\n",
    "        'Kurtosis': data.kurtosis(),\n",
    "        'Mínimo': data.min(),\n",
    "        'Máximo': data.max()\n",
    "    }\n",
    "    print(f\"Estadísticos para {label}:\")\n",
    "    for key, value in stats_dict.items():\n",
    "        print(f\"{key}: {value:.4f}\")\n",
    "    return stats_dict\n",
    "\n",
    "def compare_with_ks_rmse_mae(orig_data, imp_data):\n",
    "    \"\"\"\n",
    "    Compara distribuciones con KS-test y calcula RMSE/MAE.\n",
    "    \"\"\"\n",
    "    # Usar solo valores no nulos para KS-test\n",
    "    orig_values = orig_data.dropna().values\n",
    "    imp_values = imp_data.dropna().values  # Usar solo datos no nulos para consistencia\n",
    "    \n",
    "    # KS-test para distribuciones\n",
    "    ks_stat, ks_pvalue = stats.kstest(orig_values, imp_values)\n",
    "    print(f\"KS-test (distribuciones): stat={ks_stat:.4f}, p-value={ks_pvalue:.4f}\")\n",
    "    if ks_pvalue > 0.05:\n",
    "        print(\"Distribuciones consistentes (p>0.05)\")\n",
    "    else:\n",
    "        print(\"Distribuciones difieren significativamente\")\n",
    "    \n",
    "    # RMSE y MAE para comparación punto a punto usando índices comunes\n",
    "    orig_df = orig_data.to_frame().dropna(subset=['PRECIP'])\n",
    "    imp_df = imp_data.to_frame().dropna(subset=['PRECIP'])\n",
    "    common_dates = orig_df.index.intersection(imp_df.index)\n",
    "    \n",
    "    if len(common_dates) > 0:\n",
    "        orig_common = orig_df.loc[common_dates, 'PRECIP']\n",
    "        imp_common = imp_df.loc[common_dates, 'PRECIP']\n",
    "        rmse = np.sqrt(np.mean((orig_common - imp_common)**2))\n",
    "        mae = np.mean(np.abs(orig_common - imp_common))\n",
    "        print(f\"RMSE (error cuadrático): {rmse:.4f}\")\n",
    "        print(f\"MAE (error absoluto): {mae:.4f}\")\n",
    "    else:\n",
    "        print(\"No hay suficientes fechas comunes para calcular RMSE/MAE\")\n",
    "\n",
    "# Estadísticos originales\n",
    "orig_stats = calculate_statistics(df_original['PRECIP'], \"Originales\")\n",
    "\n",
    "# Estadísticos imputados\n",
    "imputed_stats = calculate_statistics(df_imputed['PRECIP'], \"Imputados\")\n",
    "\n",
    "# Comparación integral\n",
    "compare_with_ks_rmse_mae(df_original.set_index('FECHA')['PRECIP'], df_imputed.set_index('FECHA')['PRECIP'])\n",
    "\n",
    "# Diferencia en dispersión\n",
    "dispersion_diff = abs(orig_stats['Desviación Estándar'] - imputed_stats['Desviación Estándar'])\n",
    "print(f\"Diferencia en dispersión (std): {dispersion_diff:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2570be89-c20a-48d4-9d80-885ab2f83c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_distribution_fit_comparison(orig_data, imp_data, dist_name_orig, params_orig, dist_name_imp, params_imp):\n",
    "    \"\"\"\n",
    "    Grafica PDF y Q-Q plots comparativos para originales vs imputados.\n",
    "    \"\"\"\n",
    "    orig_data = orig_data.dropna().values\n",
    "    imp_data = imp_data.values\n",
    "    \n",
    "    x_orig = np.linspace(orig_data.min(), orig_data.max(), 100)\n",
    "    x_imp = np.linspace(imp_data.min(), imp_data.max(), 100)\n",
    "    \n",
    "    dist_orig = getattr(stats, dist_name_orig)\n",
    "    dist_imp = getattr(stats, dist_name_imp)\n",
    "    pdf_orig = dist_orig.pdf(x_orig, *params_orig)\n",
    "    pdf_imp = dist_imp.pdf(x_imp, *params_imp)\n",
    "    \n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Histograma y PDF comparativos\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(orig_data, bins=30, density=True, alpha=0.5, label='Original', color='blue')\n",
    "    plt.hist(imp_data, bins=30, density=True, alpha=0.5, label='Imputado', color='red')\n",
    "    plt.plot(x_orig, pdf_orig, 'b-', label='PDF Original')\n",
    "    plt.plot(x_imp, pdf_imp, 'r-', label='PDF Imputado')\n",
    "    plt.title('Comparación de Distribuciones')\n",
    "    plt.xlabel('PRECIP (mm)')\n",
    "    plt.ylabel('Densidad')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Q-Q plot originales\n",
    "    plt.subplot(1, 3, 2)\n",
    "    stats.probplot(orig_data, dist=dist_name_orig, sparams=params_orig, plot=plt)\n",
    "    plt.title('Q-Q Plot Originales')\n",
    "    \n",
    "    # Q-Q plot imputados\n",
    "    plt.subplot(1, 3, 3)\n",
    "    stats.probplot(imp_data, dist=dist_name_imp, sparams=params_imp, plot=plt)\n",
    "    plt.title('Q-Q Plot Imputados')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Evaluar distribuciones\n",
    "best_dist_orig, params_orig = evaluate_best_distribution(df_original['PRECIP'])\n",
    "best_dist_imp, params_imp = evaluate_best_distribution(df_imputed['PRECIP'])\n",
    "\n",
    "print(f\"Distribución original: {best_dist_orig}\")\n",
    "print(f\"Distribución imputada: {best_dist_imp}\")\n",
    "\n",
    "# Gráfico comparativo\n",
    "plot_distribution_fit_comparison(df_original['PRECIP'], df_imputed['PRECIP'], best_dist_orig, params_orig, best_dist_imp, params_imp)\n",
    "\n",
    "# Consistencia: Si tipos coinciden y KS p>0.05, son consistentes\n",
    "if best_dist_orig == best_dist_imp:\n",
    "    print(\"Tipos de distribución coinciden.\")\n",
    "else:\n",
    "    print(\"Tipos difieren; verificar con KS.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4714f2a5-9931-4756-b629-ad9d06a84dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def hydrological_validation(orig_df, imp_df):\n",
    "    \"\"\"\n",
    "    Validación específica para hidrología: Correlación, frecuencias mensuales y ajuste anual.\n",
    "    \"\"\"\n",
    "    orig_df = orig_df.set_index('FECHA')\n",
    "    imp_df = imp_df.set_index('FECHA')\n",
    "    \n",
    "    # Correlación en fechas comunes\n",
    "    common_dates = orig_df.dropna().index.intersection(imp_df.index)\n",
    "    corr = orig_df.loc[common_dates, 'PRECIP'].corr(imp_df.loc[common_dates, 'PRECIP'])\n",
    "    print(f\"Correlación Pearson (datos comunes): {corr:.4f}\")\n",
    "    \n",
    "    # Frecuencias mensuales (como en manuales CONAGUA)\n",
    "    orig_monthly = orig_df['PRECIP'].resample('ME').sum().dropna()\n",
    "    imp_monthly = imp_df['PRECIP'].resample('ME').sum().dropna()\n",
    "    monthly_corr = orig_monthly.corr(imp_monthly)\n",
    "    print(f\"Correlación en precipitación mensual: {monthly_corr:.4f}\")\n",
    "    \n",
    "    # Ajuste anual (si hay datos anuales)\n",
    "    if len(orig_monthly) >= 12:\n",
    "        orig_annual = orig_monthly.resample('YE').sum()\n",
    "        imp_annual = imp_monthly.resample('YE').sum()\n",
    "        annual_rmse = np.sqrt(np.mean((orig_annual - imp_annual)**2))\n",
    "        print(f\"RMSE en precipitación anual: {annual_rmse:.4f}\")\n",
    "\n",
    "# Aplicar validación\n",
    "hydrological_validation(df_original, df_imputed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5141350",
   "metadata": {},
   "source": [
    "# Atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717f21fd-b8bf-4bab-a559-1a0312b89a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_monthly_max_precip(df, station_id):\n",
    "    \"\"\"\n",
    "    Genera un DataFrame con precipitación mensual máxima, pivoteado por año y meses.\n",
    "    \"\"\"\n",
    "    df = df.copy()  \n",
    "    df['Año'] = df['FECHA'].dt.year\n",
    "    df['Mes'] = df['FECHA'].dt.month_name(locale='es_ES')  \n",
    "    \n",
    "    monthly_max = df.groupby(['Año', 'Mes'])['PRECIP'].max().reset_index()\n",
    "    \n",
    "    month_order = ['enero', 'febrero', 'marzo', 'abril', 'mayo', 'junio', 'julio', 'agosto', 'septiembre', 'octubre', 'noviembre', 'diciembre']\n",
    "    pivot_df = monthly_max.pivot(index='Año', columns='Mes', values='PRECIP').fillna(0)  # Rellena con 0 si no hay datos\n",
    "    \n",
    "    pivot_df = pivot_df.reindex(columns=[m.capitalize() for m in month_order])\n",
    "    \n",
    "    return pivot_df\n",
    "\n",
    "df_imputed = results[station_id]  \n",
    "\n",
    "monthly_max_df = generate_monthly_max_precip(df_imputed, station_id)\n",
    "print(\"DataFrame de Precipitación Mensual Máxima (mm):\")\n",
    "print(monthly_max_df.to_string())\n",
    "\n",
    "# Exportar a CSV si es necesario\n",
    "monthly_max_df.to_csv(f'monthly_max_precip_{station_id}.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076d0a0",
   "metadata": {},
   "source": [
    "# Atención"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f4ae76-1388-40c2-bf3d-40c229165ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la imputación con rango completo\n",
    "results_full = results\n",
    "\n",
    "df_imputed_full = results_full[station_id]\n",
    "print(f\"DataFrame imputado con rango completo para {station_id}:\")\n",
    "print(df_imputed_full.head())  # Muestra primeras filas\n",
    "\n",
    "# Opcional: Guardar\n",
    "df_imputed_full.to_csv(f'{station_id}_imputado_full_range.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c863b3-3d8e-4ab9-99e8-5b582ad3225d",
   "metadata": {},
   "source": [
    "determinar la distribucion de cada estacion y compararlas entre si calculando la desviacion estandar, R2, varianza y observar como se compartan entre si, escojer la que tenga mejor correlacion temporal y emplearla como peso para el calculo de las imputacion por los metodos antes empleados IDW ARIMA LR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c713efc-e8d9-4b0d-8b49-cd86b65f975b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cebb4d0-cb8f-41a9-bb1f-f38417325867",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2d55b-1505-455c-b198-fe53cb1cf418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63e501-0475-46d1-b41d-b377b37ae342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
